---
title: "A Machine Learning Approach to MLB Catcher Framing"
author: "Nathan Hemenway and Matthew Boyd"
date: "12/10/2021"
output: 
  beamer_presentation: default
---

```{r setup, include=FALSE}
library(tidyverse)
library(glmnet)
library(gbm)
library(randomForest)
knitr::opts_chunk$set(echo = FALSE)
load("Data/usable_data.Rda")
```

## Introduction

- "Catcher framing is the art of a catcher receiving a pitch in a way that makes it more likely for an umpire to call it a strike -- whether that's turning a borderline ball into a strike, or not losing a strike to a ball due to poor framing." - MLB.com Glossary

## Motivation

- Baseball catchers can influence the call of a ball or strike on how they catch the ball
- Some catchers are better than others at this skill
- Baseball teams are aware of this and are acquiring players good at this skill to win more games
- We want to quantify the best catcher's at framing for the 2021 season
- There are several factors that influence whether a pitch will be a strike or ball
- Catchers getting more strikes translates to more outs, and fewer runs for the opposing team
- https://baseballsavant.mlb.com/sporty-videos?playId=273b22a4-f522-4009-87d1-14176772182d

## Data

- 2021 pitch data scraped from Baseball Savant through baseballr package
- ~700,000 rows (each for a single pitch)
- Wanted to look at pitches that were not swung at by the batter (called strike or ball)
- ~350,000 rows remain

## Variables

- We used:
- Pitch type and pitch release speed, position, and spin rate
- Whether or not the pitcher and batter are right or left handed
- Count, number of outs during the at-bat, and inning number
- Where the pitch landed
- Whether the game was played home or away
- How tall the batter is

## Example

- Strike Probability by Location and Count

```{r cars, echo = FALSE, message = FALSE, cache=TRUE}
x=c(-0.95,0.95,0.95,-0.95,-0.95)
y=c(1.5,1.5,3.5,3.5,1.5)
sz=data.frame(x,y)
# Take some samples to explore
train = usable_data %>% slice(1000:150000) %>%
  mutate(strike = as.character(strike)) %>%
  select(-catcher_name)

# Train initial model

model = gbm(strike ~ plate_x + plate_z + count, 
                     data = train, n.trees = 200, distribution = "bernoulli")


# Create a grid of values
grid1 = data.frame()
x = seq(-1.8, 1.8, length.out = 125)
y = seq(1.2, 3.8, length.out = 125)
grid = expand.grid(plate_x = x, plate_z = y)

# This creates a new grid for every count factor to visualize
grid1 = grid1 %>% bind_rows(data.frame(grid, count = "0-0"),
                            data.frame(grid, count = "0-1"),
                            data.frame(grid, count = "0-2"),
                            data.frame(grid, count = "1-0"),
                            data.frame(grid, count = "1-1"),
                            data.frame(grid, count = "1-2"),
                            data.frame(grid, count = "2-0"),
                            data.frame(grid, count = "2-1"),
                            data.frame(grid, count = "2-2"),
                            data.frame(grid, count = "3-0"),
                            data.frame(grid, count = "3-1"),
                            data.frame(grid, count = "3-2")) %>%
  mutate(count = as.factor(count))


# Predict whether a pitch will be a strike or not
grid1 = grid1 %>%
  mutate(pred = predict(model, grid1, type = "response") >= 0.5)

# Visualize strikes by the count and location
ggplot()+
  geom_point(data = grid1, aes(x = plate_x, y = plate_z, color = pred)) +
  facet_wrap(~count) +
  geom_path(data = sz, aes(x = x, y = y)) +
  scale_color_manual(values = c("TRUE" = "red", "FALSE" = "white")) +
  labs(color = "Predicted Strike") +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank())
```

## Logistic Regression Model

\tiny
```{r, echo=FALSE}
data_no_catchers <- usable_data[, -18]

train <- sample(1:nrow(data_no_catchers), nrow(data_no_catchers)/2)
test <- (-train)

log_reg_fit <- glm(strike ~ ., data=data_no_catchers[train, ], family='binomial')

pred <- predict.glm(log_reg_fit, type='response', newdata = data_no_catchers[test, ])
pred[pred >= 0.5] <- 'strike'
pred[pred < 0.5] <- 'ball'

```

```{r}
conf_mx <- table(pred=pred, true=data_no_catchers[test, ]$strike)
error <- (conf_mx[2] + conf_mx[3])/length(pred)

conf_mx
1-error
```

- The variables with significant results were:
- Pitch type, release speed, z release position
- Whether the batter is a lefty/righty
- Count
- Where the pitch landed
- Most of the innings
- Batter height
- Whether it was a home game

## Ridge Regression

```{r, cache=T}
x <- model.matrix(strike ~ ., data = data_no_catchers)[, -1]
y <- data_no_catchers$strike

lambda <- seq(0.001, 10, length=100)

#Create training and test data set
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
ridge_reg_fit <-  glmnet(x[train,], y[train], alpha = 0, lambda = lambda, family='binomial')

#Choose lambda
cv_ridge <- cv.glmnet(x[train, ], y[train], alpha=0, family='binomial')
lambda_choice <- cv_ridge$lambda.min

#Get predictions
ridge_predict <- predict(ridge_reg_fit, s = lambda_choice, newx = x[test, ], type = 'class')

#Confusion matrix
ridge_conf_mx <- table(pred=ridge_predict, true=y[test])
ridge_conf_mx
error <- (ridge_conf_mx[2] + ridge_conf_mx[3])/length(ridge_predict)
1-error
```

## LASSO

```{r, cache=TRUE, message=FALSE}
#LASSO
lasso_fit <- glmnet(x[train, ], y[train], alpha = 1, lambda = lambda, family = 'binomial')
cv_lasso <- cv.glmnet(x[train, ], y[train], alpha = 1, family='binomial')

#Lambda choice
lasso_lam <- cv_lasso$lambda.min

#Get predictions
lasso_predict <- predict(lasso_fit, s = lasso_lam, newx = x[test, ], type = 'class')

#Get coefficients
lasso_coeff <- predict(lasso_fit, s = lasso_lam, type = 'coefficients')

non_zero_lasso_coef <- lasso_coeff[lasso_coeff != 0]

#Confusion matrix
lasso_mx <- table(pred=lasso_predict, true=y[test])
lasso_mx
error <- (lasso_mx[2] + lasso_mx[3])/length(lasso_predict)
1-error
```

- Some of the non-zero coefficients include:
- Pitch type
- Count
- Outs and Inning
- Batter height
- Whether the game is home or away
- Where the pitch landed

## Random Forest

- Tested several Random Forest models
  - Nodesize: 10, 30, 50, 70, 90 | # of Trees: 500
  - Compared Out-Of-Bag Error 
```{r, echo=FALSE, fig.width=7, fig.height=4}
load("Data/pitchData.Rda")
load("Model-Outputs/random-forest.Rda")

ggplot(df_rf, aes(x = nodesize, y = accuracy))+
  geom_line() +
  geom_point() +
  geom_point(data = df_rf %>% 
               filter(accuracy == max(accuracy)), aes(x = nodesize, y = accuracy), color = "red") +
  ylim(0.925, 0.935) +
  labs(x = "Nodesize",
       y = "Accuracy",
       title = "Random Forest OOB Accuracy by Nodesize") +
  theme_minimal()
```


## Boosting

- 3 Fold Cross Validation on Boosting
  - Interaction depth: 6, 11, 16 
  - Shrinkage: 0.1, 0.01, 0.001 
  - Number of Trees: 500
  - Compared CV accuracy
- 27 models took over 5 hours to run
  
```{r, echo=FALSE, fig.width=6, fig.height=2.5}
load("Model-Outputs/boosting.Rda")

ggplot(df_gbm_final, aes(x = interaction_depth, y = accuracy, color = as.character(lambda))) +
  geom_line() +
  theme_minimal()+
  geom_point() +
  geom_point(aes(x = 16, y = 0.935), color = "red") +
  labs(x = "Accuracy", y = "Interactin Depth",
       color = "Shrinkage",
       title = "Boosting Accuracy by Interaction Depth and Shrinkage")
```


## Boosting Part 2

- Only tuned shrinkage and interaction depth
- Now tune trees with interaction depth of 16 and shrinkage of 0.1
- Trees: 300, 500, 600, 900, 1200

```{r,  fig.width=6, fig.height=3.2}
load("Model-Outputs/boosting2.Rda")
final_df_gbm %>%
  group_by(trees) %>%
  summarize(accuracy = mean(accuracy)) %>%
  arrange(desc(accuracy)) %>%
  ggplot(aes(x = trees, y = accuracy)) +
  geom_line(color = "goldenrod") +
  theme_minimal() +
  labs(x = "Number of Trees",
       y = "Accuracy",
       title = "Boosting Accuracy by # of Trees")
```

## Model Comparison

- Boosting with interaction depth of 16, shrinkage of 0.1, and 500 trees has the highest accuracy

```{r}
df_rf %>%
  rename(parameter = nodesize) %>%
  bind_rows(final_df_gbm %>%
              group_by(trees) %>%
              summarize(accuracy = mean(accuracy)) %>%
              arrange(desc(accuracy)) %>%
              mutate(model = "boosting") %>%
              select(model, accuracy, trees) %>%
              rename(parameter = trees)) %>%
  arrange(desc(accuracy))
```

## Results

- To give credit to each catcher, we use this equation for every pitch:
  - If strike: 1 - Strike Probability
  - If ball: Strike Probability * -1
  
  