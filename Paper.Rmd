---
title: "A Machine Learning Approach to MLB Catcher Framing"
author: "Nathan Hemenway and Matthew Boyd"
date: "12/16/2021"
output: pdf_document
---


<style type="text/css">
  body{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE, cache=T, echo=FALSE}
library(tidyverse)
library(glmnet)
library(gbm)
library(randomForest)
library(caret)
library(knitr)
knitr::opts_chunk$set(echo = FALSE)
load("Data/usable_data.Rda")
x=c(-0.95,0.95,0.95,-0.95,-0.95)
y=c(1.5,1.5,3.5,3.5,1.5)
sz=data.frame(x,y)
```

# Motivation
 
Catcher framing is the art of making a pitch look better than it really is. This is a skill a catcher can have to convince the umpire to call a pitch that would otherwise be called a ball, a strike. If a pitch is for all intents and purposes a ball, then the batter likely will not swing at it. Then if the catcher can make it appear like a strike to the umpire, the umpire will call it a strike and bring the batter one step closer to an out. This is an obvious advantage, since the more strike outs take place, the less of opportunity the opposing team has to score runs while at-bat. 

Our idea to measure this phenomenon is to use statistical machine learning methods in order to predict whether a given pitch is a strike. Given the nature of baseball, there will be pitches that are likely strikes and likely balls. If a catcher can convince the umpire to call a likely ball a strike, then that catcher is a good catcher and gets credited for that pitch. On the other hand, if the umpire calls what should have been a strike a ball, then the catcher did not do a good job and is penalized for the pitch. We sought to find out which catchers are best at pitch framing by predicting how likely the pitches thrown are to be strikes, and then seeing how well the catchers can turn them into strikes.

# Methodology

Our methodology is to use available data to predict whether a pitch will be a strike for all data points up until the catcher catches the ball. This will give us a good idea of whether any pitch, given a number of characteristics, will be called a strike or called a ball. From the model output, we can get a probability that every pitch will be a strike and compare that to the observed outcome of the pitch. From here we can find out which catchers during the 2021 season excelled at getting more called strikes than their probabilities say they would.   

## Data

The data contains pitch characteristics for every pitch thrown during the 2021 Major League Baseball season. It was scraped from [Baseball Savant](https://baseballsavant.mlb.com/) using the [baseballr](https://billpetti.github.io/baseballr/) package. The scraped data contains ~700,000 rows with ~90 columns. We are only interested in pitches that were a called strike or a ball. This is because if the batter swung at a pitch, how the catcher presents the pitch doesn't matter. This left ~350,000 rows to model from.

## Variables

Of the 93 variables to choose from, we chose the ones listed below. The reason we chose these variables was because we thought they could all have an impact on whether a pitch will be a called strike or not.   
  - Pitch type, pitch release speed, release position, and spin rate  
  - Whether or not the pitcher and batter are right or left handed  
  - Count, number of outs during the at-bat, and inning number  
  - Where the pitch landed  
  - Whether the game was played home or away  
  - How tall the batter is  


Several variables in the data set also contained mostly NA values and those columns were ignored in modeling.


# Models

We chose to predict whether a pitch is a strike on several models to find the model with the highest accuracy. The model chosen will be the model used on the results.

### Logistic Regression

```{r, cache = T, warning=FALSE, message=FALSE, echo=FALSE}
set.seed(445)
load("Data/usable_data.Rda")
data_no_catchers <- usable_data[, -18]

train <- sample(1:nrow(data_no_catchers), nrow(data_no_catchers)/2)
test <- (-train)

log_reg_fit <- glm(strike ~ ., data=data_no_catchers[train, ], family='binomial')
summary(log_reg_fit)

pred <- predict.glm(log_reg_fit, type='response', newdata = data_no_catchers[test, ])
pred[pred >= 0.5] <- 'strike'
pred[pred < 0.5] <- 'ball'

conf_mx <- table(pred=pred, true=data_no_catchers[test, ]$strike)
error <- (conf_mx[2] + conf_mx[3])/length(pred)

conf_mx
1-error
```

We started with a logistic regression classifier using the selected variables as a preliminary model for predicting strikes. The model did not perform very impressively, with a prediction accuracy of 0.67. The summary for the logistic regression model gives which predictors are statistically significant, which included: 
- Pitch type, release speed, z release position  
- Whether the batter is a lefty/righty  
- Count  
- Where the pitch landed  
- Most of the innings  
- Batter height  
- Whether it was a home game  



### Ridge Regression

```{r, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE}
set.seed(445)
library(glmnet)

x <- model.matrix(strike ~ ., data = data_no_catchers)[, -1]
y <- data_no_catchers$strike

lambda <- seq(0.001, 10, length=100)

#Create training and test data set
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
ridge_reg_fit <-  glmnet(x[train,], y[train], alpha = 0, lambda = lambda, family='binomial')

#Choose lambda
cv_ridge <- cv.glmnet(x[train, ], y[train], alpha=0, family='binomial')
lambda_choice <- cv_ridge$lambda.min

#Get predictions
ridge_predict <- predict(ridge_reg_fit, s = lambda_choice, newx = x[test, ], type = 'class')

#Confusion matrix
ridge_conf_mx <- table(pred=ridge_predict, true=y[test])
ridge_conf_mx
error <- (ridge_conf_mx[2] + ridge_conf_mx[3])/length(ridge_predict)
1-error
```
The ridge regression model performed modestly with a prediction accuracy of 0.672. This is hardly any different from the logistic regression model, so it also is not the best choice of model to be used to rank the catchers.

### LASSO

```{r, cache=TRUE, warning=FALSE, message=FALSE, echo=FALSE}
set.seed(445)
lasso_fit <- glmnet(x[train, ], y[train], alpha = 1, lambda = lambda, family = 'binomial')
cv_lasso <- cv.glmnet(x[train, ], y[train], alpha = 1, family='binomial')

#Lambda choice
lasso_lam <- cv_lasso$lambda.min

#Get predictions
lasso_predict <- predict(lasso_fit, s = lasso_lam, newx = x[test, ], type = 'class')

#Get coefficients
lasso_coeff <- predict(lasso_fit, s = lasso_lam, type = 'coefficients')
lasso_coeff

non_zero_lasso_coef <- lasso_coeff[lasso_coeff != 0]
#Confusion matrix
lasso_mx <- table(pred=lasso_predict, true=y[test])
lasso_mx
error <- (lasso_mx[2] + lasso_mx[3])/length(lasso_predict)
1-error
```

The model obtained using the LASSO gave a similar prediction accuracy as the logistic and ridge regression models at 0.673. The LASSO also performs variable selection, and gave non-zero coefficients for:
  - Pitch type  
  - Count  
  - Outs and Inning  
  - Batter height  
  - Whether the game is home or away  
  - Where the pitch landed  


### Random Forest

The next step for modeling involved fitting the data on tree type model. The first model tested was random forest. With the same variables used above, we fit a random forest model on the data with a nodesize of 10, 30, 50, 70, and 90. We kept the number of trees the same at 500. After each model was fit, we took the out-of-bag error rate and compared the models to find the most accurate. More values for nodesize would have been considered but each model took a long time to run. Running these 5 models took about 64 minutes total.
  
```{r, echo=FALSE, fig.width=7, fig.height=4, cache=TRUE}
load("Data/pitchData.Rda")
load("Model-Outputs/random-forest.Rda")

ggplot(df_rf, aes(x = nodesize, y = accuracy))+
  geom_line() +
  geom_point() +
  geom_point(data = df_rf %>% 
               filter(accuracy == max(accuracy)), aes(x = nodesize, y = accuracy), color = "red") +
  ylim(0.925, 0.935) +
  labs(x = "Nodesize",
       y = "Accuracy",
       title = "Random Forest OOB Accuracy by Nodesize") +
  theme_minimal()
```

As you can see from the image, the random forest model with a nodesize of 10 had the highest OOB accuracy. One thing to note is how much more accurate the random forest models performed than logistic regression, ridge regression, and LASSO. This shows how powerful growing several trees are on prediction rate.

### Boosting

A similar process was performed for boosting with a few adjustments. Instead of using the OOB error, we performed 3-fold cross validation to tune parameters on interaction depth and shrinkage. The reason  we only chose 3-fold cross validation was because the models would take a long time to run. We tested interaction depth's of 6, 11, and 16 with shrinkage of 0.1, 0.01, 0.001. The number of trees was set to 500 for all models. This process performed 27 models which took over 5 hours to run. Once all models were finished, we compared the cross validation accuracy with the plot below.  
  
```{r, echo=FALSE, fig.width=6, fig.height=2.5, warning=FALSE, message=FALSE}
library(tidyverse)
load("Model-Outputs/boosting.Rda")

ggplot(df_gbm_final, aes(x = interaction_depth, y = accuracy, color = as.character(lambda))) +
  geom_line() +
  theme_minimal()+
  geom_point() +
  geom_point(aes(x = 16, y = 0.935), color = "red") +
  labs(y = "Accuracy", x = "Interactin Depth",
       color = "Shrinkage",
       title = "Boosting Accuracy by Interaction Depth and Shrinkage")
```

The model that performed best had a interaction depth of 16 with shrinkage rate of 0.1. Now that we have the best model, it's now time to again perform 3-fold cross validation on the number of trees. We tested trees of 300, 600, 900, and 1200 with shrinkage of 0.1 and interaction depth of 16. Below show the results of the cross validation accuracy by number of trees.

```{r,  fig.width=6, fig.height=3.2, echo=FALSE, message=FALSE, warning=FALSE}
load("Model-Outputs/boosting2.Rda")
final_df_gbm %>%
  group_by(trees) %>%
  summarize(accuracy = mean(accuracy)) %>%
  arrange(desc(accuracy)) %>%
  ggplot(aes(x = trees, y = accuracy)) +
  geom_line(color = "goldenrod") +
  theme_minimal() +
  labs(x = "Number of Trees",
       y = "Accuracy",
       title = "Boosting Accuracy by # of Trees")
```

500 trees was used in the initial 3-fold cross validation and it shows the highest accuracy among all boosting models. 

### Model Comparison

We now compare all tree methods used to find the model with the highest accuracy. These models all performed at a much higher accuracy rate than logistic regression, ridge regression, and LASSO. Below are all the models sorted by highest accuracy.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
df_rf %>%
  rename(parameter = nodesize) %>%
  bind_rows(final_df_gbm %>%
              group_by(trees) %>%
              summarize(accuracy = mean(accuracy)) %>%
              arrange(desc(accuracy)) %>%
              mutate(model = "boosting") %>%
              select(model, accuracy, trees) %>%
              rename(parameter = trees)) %>%
  arrange(desc(accuracy)) %>%
  kable
```

As you can see, boosting with shrinkage rate of 0.1, interaction depth of 16, and 500 trees performed the best with a 93.5%. This model will be used on the results.

### Variable Importance

```{r, message=FALSE,echo=FALSE, warning=FALSE}
library(gbm)
load("Final-Model/final_boosting_model.Rda")
var = summary.gbm(final_model, plotit = F)
ggplot(var, aes(x = fct_reorder(var, rel.inf), y = rel.inf, fill = rel.inf)) +
  geom_col() +
  coord_flip() +
  labs(y = "Relative Influence",
       title = "Predictor Variable Importance") +
  theme_minimal()+
  theme(axis.title.y = element_blank()) +
  guides(fill = F)
```

Looking at the variable importance for whether a pitch will be a strike on the best model, the location of the pitch is the most important (plate_x and plate_z) with the count of the pitch and height of th batter (sz_top and sz_bot). Most variables have very little importance compared to the location of the pitch. The handedness of the pitcher (righty/lefty) and the location (home/away) have the lowest importance of all variables.

# Results

To give credit to each catcher, we use this equation for every pitch:  
  
 - If observed strike: 1 - Strike Probability  
 - If observed ball: Strike Probability * -1  
 
To give an example, below is a plot of the strike zone. Umpires will try to make a call of a strike or ball if the pitch is located inside the rectangle. However, umpires don't have access to a strike zone while actually making calls themselves, so there will be some error as to whether a pitch an umpire calls a strike, is actually a strike (inside the strike zone).  

The point located inside the strike zone is a theoretical location of a pitch with a 0.9 strike probability. That's quite high but it makes sense, given the pitch is in the strike zone. The pitch outside of the strike zone has a 0.2 strike probability since the pitch lands outside the strike zone. If the pitch with a 0.9 strike probability gets called a strike, the catcher will earn 1 - 0.9 = 0.1 strikes worth of credit. This is a small contribution given that the pitch is more likely to be a strike anyway. However, if the pitch gets called a ball, he will lose 0.9 strikes worth of credit which is a significant loss. That is because the catcher could have could have presented the pitch to the umpire very poorly, and he deserves to be be given credit with losing a strike for his team. If we look at the pitch out of the zone, if that pitch is an observed called strike, the catcher will receive 1 - 0.2 = 0.8 strikes worth of credit. That's a big gain for the catcher's team because he turned a pitch, more likely going to be a ball, into a strike and he should be rewarded. If that pitch is an observed ball, he will only lose -0.2 strikes worth of credit. That's small because the pitch is likely to be a ball anyway.  
 
```{r, fig.width=2, fig.height=2, cache=TRUE, echo=FALSE}
ggplot()+
  geom_path(data = sz, aes(x = x, y = y)) +
  xlim(-1.8, 1.8) +
  ylim(0.5, 5) +
  coord_equal() +
  geom_point(aes(x = .5, y = 2.5)) +
  geom_text(aes(x = 0.6, y = 2.8, label = "0.9")) +
  geom_point(aes(x = -.5, y = 4)) +
  geom_text(aes(x = -0.4, y = 4.3, label = "0.2")) +
  theme_minimal()+
  theme(axis.title.x = element_blank(),
        axis.text.x = element_blank(),
        axis.title.y = element_blank(),
        axis.text.y = element_blank()) 
```

This approach is completed for every pitch in th 2021 Major League Baseball season since we will have a strike probability and we will know the observed outcome of the pitch (called strike or ball). This way we can see which catcher's are better at presenting pitches to the umpire to benefit their team. 

```{r, message=FALSE, echo=FALSE}
load("Data/pitchData.Rda")
# Analysis
data = data %>%
  mutate(strike_prob = predict(final_model, data, type = "response"),
         strike = as.numeric(as.character(strike)))

data = data %>%
  mutate(credit = case_when(
    strike == 1 ~ 1 - strike_prob,
    strike == 0 ~ strike_prob * -1))
```

```{r, echo=FALSE}
# Leader board
data %>%
  group_by(catcher_name) %>%
  summarize(strikes_above_avg = sum(credit),
            pitches_caught = n(),
            strikes_aa_100 = strikes_above_avg / (pitches_caught / 100)) %>%
  filter(pitches_caught > 2000) %>%
  arrange(desc(strikes_above_avg)) %>% head(10) %>%
  rename("Catcher Name" = catcher_name,
         "Strikes Above Average" = strikes_above_avg,
         "Pitches Caught" = pitches_caught,
         "Strikes Above Average Per 100 Pitches Caught" = strikes_aa_100) %>% 
  kable

data %>%
  group_by(catcher_name) %>%
  summarize(strikes_above_avg = sum(credit),
            pitches_caught = n(),
            strikes_aa_100 = strikes_above_avg / (pitches_caught / 100)) %>%
  filter(pitches_caught > 2000) %>%
  arrange(strikes_above_avg) %>% head(10) %>%
  rename("Catcher Name" = catcher_name,
         "Strikes Above Average" = strikes_above_avg,
         "Pitches Caught" = pitches_caught,
         "Strikes Above Average Per 100 Pitches Caught" = strikes_aa_100) %>% 
  kable
```

The tables above show the top 10 framing catchers and the bottom 10 framing catchers. The top three include Max Stassi of the Los Angeles Angels, Jose Trevino of the Texas Rangers, and J.T. Realmuto of the Philidephia Phillies. The bottom three include Salvador Perez of the Kansas City Royals, Pedro Severino of the Baltimore Orioles, and Zack Collins of the Chicago White Sox. To interpret this, Max Stassi gained his team 99.2 strikes above average just from how he presents the ball to the umpire. Salvador Perez lost his team 102.6 strikes below average from how he presents the ball to the umpire. That's a difference of about 200 strikes between the bottom and top, a large difference.


# Conclusion

How catchers catch the ball seems unimportant to the naked eye. An umpire is supposed to call a strike if the pitch location is within a certain range but since umpires are human, they can be wrong. Our analysis shows that some catcher's add value to their team from the small skill of how they catch the ball. The next time you watch a baseball game, just know that how a catcher catches the ball can have a greater impact on the game than anyone would think.

# References

A. Liaw and M. Wiener (2002). Classification and Regression by randomForest. R News 2(3), 18--22

“Baseball Savant: Trending MLB Players, Statcast and Visualizations.” Baseballsavant.com,
    https://baseballsavant.mlb.com/. 
  
“Catcher Framing: Glossary.” MLB.com, https://www.mlb.com/glossary/statcast/catcher-framing. 

Bill Petti (2021). baseballr: Functions for Acquiring and Analyzing Baseball Data.
  https://billpetti.github.io/baseballr/, https://github.com/BillPetti/baseballr/.

Brandon Greenwell, Bradley Boehmke, Jay Cunningham and GBM Developers (2020). gbm: Generalized Boosted
  Regression Models. R package version 2.1.8. https://CRAN.R-project.org/package=gbm
  
Jerome Friedman, Trevor Hastie, Robert Tibshirani (2010). Regularization Paths for Generalized Linear
  Models via Coordinate Descent. Journal of Statistical Software, 33(1), 1-22. URL
  https://www.jstatsoft.org/v33/i01/.
  
Max Kuhn (2021). caret: Classification and Regression Training. R package version 6.0-90.
  https://CRAN.R-project.org/package=caret

Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686,  
        https://doi.org/10.21105/joss.01686
  
Yihui Xie (2021). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version
  1.36.
  
  
